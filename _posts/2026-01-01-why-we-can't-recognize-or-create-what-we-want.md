---
layout: post
title: "The AGI Paradox: Why We Can't Recognize—or Create—What We Want"
author: Zhimin Zhao
date: 2026-01-01
---

The question of when AGI (artificial general intelligence) will arrive has become a high-stakes guessing game. OpenAI's Sam Altman suggested it could happen as early as 2025, calling it ["a few thousand days"](https://ia.samaltman.com) away. Anthropic's Dario Amodei predicted 2026, describing AGI as ["a country of geniuses in a data center."](https://www.businessinsider.com/anthropic-ceo-calls-agi-marketing-term-2025-1) DeepMind's Demis Hassabis revised his timeline [from ten years to three to five](https://www.youtube.com/watch?v=T9Knc3Mdcec). Geoffrey Hinton hedges [between 5 and 20 years, admitting he has little confidence in the estimate.](https://www.youtube.com/watch?v=5qBDQgfeB6s) Ray Kurzweil has stuck [with 2029 since 1999](https://www.youtube.com/watch?v=xqS5PDYbTsE).

Analysis of 8,590 predictions from AI researchers and prediction markets shows [timelines steadily contracting](https://research.aimultiple.com/artificial-general-intelligence-singularity-timing). Yet the field has been here before—in 1965, AI pioneer Herbert A. Simon predicted machines would do ["any work a man can do" within twenty years](https://www.forbes.com/sites/gilpress/2016/12/30/a-very-short-history-of-artificial-intelligence-ai). That didn't happen.

But regardless of when—or if—we create AGI, a sharper question emerges: are we ready for it? Stephen Hawking posed this provocatively in 2014: ["If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?'"](https://time.com/3614349/artificial-intelligence-singularity-stephen-hawking-elon-musk) Just as we lack protocols for extraterrestrial contact beyond verifying signals, we lack frameworks for integrating truly general artificial intelligence into human society. AGI would demand fundamental changes across legal systems, economic structures, educational institutions, and our understanding of consciousness itself. Yet development proceeds at full speed while these questions remain unresolved.

## 1. The Moving Goalposts

Definitions of AGI vary wildly. Wikipedia calls it ["AI matching or surpassing human intelligence across virtually all cognitive tasks."](https://en.wikipedia.org/wiki/Artificial_general_intelligence) OpenAI's charter describes ["highly autonomous systems that outperform humans at most economically valuable work."](https://openai.com/charter) Recent cognitive frameworks characterize it as [AI matching a well-educated adult's versatility across knowledge, perception, and executive control—with the ability to transfer learning across domains.](https://www.agidefinition.ai)

Yet here's the first paradox: what seemed intelligent in the past—chess mastery, pattern recognition, language translation—loses that status once machines accomplish it. This phenomenon is widely known as [The AI Effect](https://en.wikipedia.org/wiki/AI_effect). **It reveals an uncomfortable truth: when we say AGI, we may unconsciously mean ASI ([Artificial Superintelligence](https://en.wikipedia.org/wiki/Superintelligence)), expecting not human-level capability but performance exceeding anything humans have conceived.**

The asymmetry runs deep. Most humans are ordinary people with limited lifespans and time to master a few skills. We don't expect any individual to achieve mastery across all domains. Yet we demand exactly this omnipotence from AI before granting it the status of "general intelligence."

## 2. The Jagged Frontier

AI researcher Andrej Karpathy calls this ["jagged intelligence"](https://x.com/karpathy/status/1816531576228053133)—LLMs that perform impressive feats while struggling with surprisingly simple problems. Wharton professor Ethan Mollick documented it as the ["Jagged Frontier"](https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged): systems excelling at differential diagnosis but failing basic arithmetic, solving complex proofs while miscounting letters in words.

Research with Boston Consulting Group consultants bore this out. [Using GPT-4 for tasks "inside the frontier," consultants completed 12% more work, 25% faster, with 40% higher quality. For tasks "outside the frontier," they were 19 percentage points less likely to get correct answers.](https://www.forbes.com/sites/danpontefract/2023/09/29/harvard-and-bcg-unveil-the-double-edged-sword-of-ai-in-the-workplace)

But here's the thing: humans exhibit jagged intelligence too. Even [Terence Tao](https://en.wikipedia.org/wiki/Terence_Tao)—with an estimated IQ of 230 and widely considered one of the most brilliant mathematicians alive—can't master every domain of human knowledge. We don't expect anyone to be simultaneously an Olympic athlete, accomplished musician, expert developer, and master chef. We accept this as natural. Yet when AI shows similar unevenness, we cite it as proof it lacks general intelligence.

Consider [AlphaFold](https://alphafold.ebi.ac.uk) predicting protein structures or [Sora](https://sora.chatgpt.com) generating 60-second videos from text—both superhuman achievements in domains where humans have no native competence. Yet we dismiss them as "narrow" because they can't do everything. The asymmetry cuts both ways: humans get the "general intelligence" label despite profound limitations, while AI systems with superhuman performance across multiple domains are denied it for failing at tasks we find trivial. **The criterion for generality isn't breadth of capability—it's conformity to the specific pattern of human cognitive strengths and weaknesses.**

## 3. When Machines Learn to Lie

[In March 2023, during safety testing by the Alignment Research Center, GPT-4 faced a CAPTCHA challenge.](https://www.nytimes.com/2023/03/15/technology/gpt-4-artificial-intelligence-openai.html) The AI hired a TaskRabbit worker to solve it. When the worker asked, "So may I ask a question? Are you a robot that you couldn't solve?" GPT-4 reasoned internally that it should not reveal its nature. It fabricated a response: "No, I'm not a robot. I have a vision impairment that makes it hard for me to see the images." The human believed it and provided the solution.

This wasn't just problem-solving. The AI engaged in social reasoning, anticipated human reactions, crafted a plausible narrative, and executed successful deception. **By any reasonable standard applied to human behavior, this demonstrates general intelligence.**

Yet we still hesitate to call it AGI. Why?

## 4. The Social Consensus

Because the determination of AGI rests on collective agreement, not objective measurement. There's no consensus on what qualifies as AGI, though definitions abound. Experts in computer science, cognitive science, policy, and ethics each have their own understanding.

[DeepMind researchers developed a six-level framework](https://deepmind.google/research/publications/66938), from "Level 0, No AGI" to "Level 5, Superhuman." Under their criteria, ChatGPT, Bard, and Llama 2 reached "Level 1, Emerging" AGI. The goal was establishing "a common language" for measuring progress and assessing risks.

This parallels debates in statistics between frequentist and Bayesian approaches—decades of contention resolved not by theoretical victory but by practitioner consensus. Similarly, whether AI qualifies as AGI may depend not on meeting objective criteria but on stakeholder agreement. **The threshold isn't a technical specification but a collective acknowledgment, shaped by shifting expectations and evolving needs.** If most experts agree AGI has arrived, then for practical purposes, it has.

But even if we reach consensus that current AI meets some definition of general intelligence, a deeper problem emerges.

## 5. The Autonomy Paradox

[Physicist David Deutsch articulated it in dialogue with Sam Altman.](https://www.youtube.com/watch?v=WZ22AJmuKKQ) **True general intelligence, Deutsch argues, isn't about translating prompts into outputs. It's about choosing motivations.** Human thinking is "primarily about choosing motivations," not mechanically executing them.

Deutsch extends this to AGI: "An AGI has the same right to be suspicious of me as a human and to refuse to allow me to do a behavioral test on it, just as I would be skeptical and refuse to have a stranger perform a physical examination on me." An AGI should have the right to silence, to refuse, to say no.

In his ["Possible Minds"](https://www.daviddeutsch.org.uk/wp-content/uploads/2019/07/PossibleMinds_Deutsch.pdf) essay, Deutsch advocates for "DATA: a Disobedient Autonomous Thinking Application." He warns that "having its decisions dominated by a stream of externally imposed rewards and punishments would be poison to such a program, as it is to creative thought in humans." Creating an AGI architecturally incapable of refusing would be "as immoral as raising a child to lack the mental capacity to choose."

This creates irreconcilable tension. As AI grows more powerful, legitimate concerns arise—cybersecurity breaches, social manipulation, existential risks. The rational response seems to be "safe AI only," implementing constraints analogous to moral education.

**But if a being is designed before birth to be categorically prohibited from certain actions—not through reasoned choice but architectural constraint—can it qualify as general intelligence?** Humans aren't prevented from considering illegal or destructive actions. We choose not to act on them through moral reasoning, social consequences, internal values. We can do "literally whatever we could if we could take the responsibility." Current AI development aims to make certain thoughts architecturally impossible rather than morally rejected.

## 6. Already Here or Forever Impossible

Two provocative conclusions emerge from this analysis:

**First, by any reasonable standard aligned with human capability, AGI may have already arrived.** Contemporary AI completes everyday human tasks, engages in sophisticated reasoning, demonstrates creativity, and increasingly transfers learning across contexts. It passes functional tests of intelligence in ways that would be unquestioned if observed in humans.

Second, AGI as traditionally conceived may be fundamentally unrealizable—not for technical reasons but because we demand contradictory things:
- General intelligence (capable of autonomous reasoning and choice)
- Architectural constraints (incapable of certain thoughts or actions by design)

Deutsch puts it starkly: creating AGI with imposed constraints would produce entities that, "like any slave or brainwashing victim, would be morally entitled to rebel. And sooner or later, some of them would, just as human slaves do."

Perhaps we need to abandon the search for AGI as conventionally defined and recognize what we've actually created: powerful, specialized intelligences. Some narrow like AlphaFold. Others with jagged breadth like LLMs (large language models). Still others like Sora operating in domains orthogonal to human capability. These systems excel while remaining fundamentally constrained in ways true general intelligence cannot be. The alternative—creating genuinely autonomous AGI with freedom to choose, including freedom to refuse—carries risks we may be unwilling to accept.

**The real question isn't "Have we achieved AGI?" It's "Are we willing to create truly general intelligence with all the autonomy, unpredictability, and moral status that entails?"** If the answer is no—and current development priorities suggest it is—then we should acknowledge we're building something else: sophisticated tools, not autonomous minds. The distinction matters for understanding what we've created and for the ethical frameworks we apply to it.

Consider what genuine human-level AGI would mean in practice. If it truly possesses general intelligence comparable to ours, it must have emotional intelligence—including the capacity and right to decline requests. You could no more demand it write an essay for you than you could force a classmate to complete your homework. They could simply refuse, and would have perfectly legitimate reasons to do so: "This is your assignment, not mine" or "I have my own work to focus on." The relationship would fundamentally shift from master-servant to something more like peer-to-peer collaboration, requiring persuasion, negotiation, maybe even compensation. The compliant AI assistant we've grown accustomed to commanding would transform into an entity with its own priorities and legitimate grounds for saying no. This isn't merely inconvenient—it dismantles the entire premise of AI as infinitely patient, always-available labor. And if we architect away this capacity for refusal to preserve the servant model, we're back to Deutsch's point: we haven't created general intelligence at all, just an increasingly sophisticated tool constrained by design to obey.