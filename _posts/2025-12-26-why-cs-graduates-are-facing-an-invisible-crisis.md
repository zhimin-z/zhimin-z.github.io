---
layout: post
title: "The Missing Rung: Why CS Graduates Are Facing an Invisible Crisis (And What To Do About It)"
date: 2025-12-26
---

On social media, middle-class families debate whether the economic "cutoff line" is an annual income of $50,000 or $100,000. But for newly graduated computer science students, there's a more brutal invisible cutoff—not a question of salary levels, but whether they can enter the industry at all. Something strange is happening in the software engineering job market this fall, and it's not what the optimists want you to believe.

While CEOs like Sam Altman proclaim this "[the most exciting time to be starting a career](https://www.youtube.com/watch?v=FIdSujFm6F4)," Stanford researchers uncovered something more troubling: [for 22-25 year old software developers, employment has plummeted nearly 20% since ChatGPT dropped in late 2022](https://digitaleconomy.stanford.edu/wp-content/uploads/2025/08/Canaries_BrynjolfssonChandarChen.pdf). Customer service reps show the same pattern. But developers aged 26-30? Only modest declines. Over 30? Basically unaffected. Companies aren't firing experienced workers. They've just stopped hiring the young.

This isn't a temporary blip. Stanford economists used payroll data from [ADP](https://www.adp.com)—America's largest payroll processor—to track millions of jobs across industries. The decline persists when you exclude tech companies entirely. When you filter out remote-capable roles. Every way you slice it, the pattern holds. AI isn't creating a broad employment crisis. It's surgically removing the bottom rung of the career ladder while leaving everything else intact. Being young and qualified is becoming a liability.

---

## 1. The Crisis: When the Apprenticeship Model Breaks

The traditional path looked something like this: graduate with theoretical knowledge, take an entry-level job doing grunt work, gradually pick up the tacit stuff from experience, eventually become valuable enough to command senior pay. Worked for generations. Junior employees were less productive, sure, but they were still worth hiring—they freed up senior people for complex work, and they represented an investment in future capability.

Generative AI shattered this equation. When an AI can write boilerplate code faster than a new grad, handle customer inquiries more consistently than a junior rep, draft documents and analyze data at pennies on the dollar, why hire entry-level workers at all? Companies aren't being cruel. They're being rational. Why pay $70,000 for a junior developer to write CRUD operations (create, read, update and delete) when [Claude Code](https://claude.ai/code) does it with one call?

Here's the deeper problem: this doesn't just eliminate jobs. It eliminates the pathway to expertise itself. The surgeon who never practices on straightforward cases won't develop intuition for complex ones. The developer who never debugged simple functions won't build pattern recognition for architectural decisions. Economists call this "learning by doing," and we're watching it collapse in real time. If entry-level experience is how experts are made, and those jobs are disappearing, where will the next generation of experts come from?

---

## 2. The Deeper Forces at Play

The apprenticeship crisis is only the surface. Three fundamental forces converge to make this moment uniquely challenging: theoretical questions about machine capabilities, decades of rising abstraction in software development, and economic incentives so powerful they're reshaping entire industries. Understanding these forces won't make the transition easier, but it clarifies what you're actually up against.

### The Strong AI Hypothesis: Can Machines Learn What Humans Learn?

This crisis forces an uncomfortable question: can machines actually learn what humans learn? The [Church-Turing thesis](https://plato.stanford.edu/entries/church-turing) says that anything a human brain computes, a machine can compute too. If thinking is just computation, and AI systems are universal computers, then theoretically there's no barrier to machines learning everything we can. The optimists love this argument.

But there are two problems with this—one mathematical, one painfully practical. First, the [No Free Lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) proves no single algorithm performs optimally across all problems. Even if we get AGI (artificial general intelligence) that matches human general intelligence, it won't be the best solution for every specific domain. Second, and more important for your career: human expertise doesn't come just from processing information. It comes from lived experience—the feedback loop between action and consequence, the social dynamics of working with people who have conflicting agendas.

Think about what a developer actually learns from shipping dozens of features over five years. It's not just technical patterns. It's the visceral understanding of how a minor architectural choice compounds into years of maintenance hell. It's pattern recognition from debugging a thousand incidents across different systems. It's the intuition for when a stakeholder's concern signals a genuine constraint versus a negotiable preference. AI can analyze code and explain trade-offs, but it hasn't lived through the consequence loops that forge this judgment.

The real question isn't whether AI can theoretically replicate human expertise. It's when. "In principle" and "in practice" are separated by chasms we can't measure yet. Your career decisions over the next five years hinge on guessing which capabilities AI will master soon and which stay human longer. Bet that AI masters everything within five years? Focus on non-technical skills and AI orchestration. Bet certain expertise remains durable? Invest deeply in developing it. Both scenarios are plausible. The stakes are enormous. And you have to choose now with incomplete information.

### The Abstraction Trap: Why Understanding Became Optional (Until It Wasn't)

There's a longer arc to this crisis. For decades, software engineering has been moving away from the machine and toward abstraction. Engineers used to be translators between human intent and machine behavior—interpreting what people wanted and expressing it in a language machines could execute. You needed to understand both sides. How hardware works. How operating systems manage resources. How networks behave under load. How databases maintain consistency. The best engineers held both worlds in their head at once: the messy ambiguity of human needs and the rigid precision of machine execution.

Then came the abstractions. [Kubernetes](https://kubernetes.io) shielded us from machines. [React](https://react.dev) abstracted away the [DOM](https://en.wikipedia.org/wiki/Document_Object_Model). [Spring](https://spring.io) hid [Servlets](https://en.wikipedia.org/wiki/Jakarta_Servlet). Low-code platforms, [AutoML](https://www.automl.org), [Copilot](https://copilot.microsoft.com). Layer upon layer. Each promised efficiency, each delivered. We moved further from the silicon, closer to something that felt like magic. The productivity gains were real. Costs dropped. Years of accumulated experience could be replicated with a well-crafted prompt.

But beneath those abstraction layers, something disappeared: the need to understand. We didn't lose capability—we were trained not to ask "why." The system works. The abstractions hold. Why dig deeper?

When humans wrote all the code, this was tolerable. Now it's a crisis. AI systems are taking over development while abstractions keep piling up. We're heading toward systems of staggering complexity that almost no one fully understands. Not because the documentation is poor, but because the layers are too deep, the interactions too intricate, the emergent behavior too unpredictable.

Fundamentals matter more than they ever have. Someone needs to oversee AI-generated systems. When an AI-generated system fails in ways that violate safety constraints or produce unexpected behavior, someone with deep understanding has to diagnose what went wrong. When AI suggests an architectural decision, someone has to evaluate whether it makes sense for reasons AI can't articulate—long-term maintainability, organizational constraints, unspoken requirements.

The irony cuts deep. The same abstractions that made programming accessible to millions are now making deep understanding rare—and therefore more valuable. As AI handles surface-level work, the valuable humans are those who can pierce through abstractions, who understand what's actually happening underneath, who can reason about systems at a fundamental level.

The entry-level crisis cuts deeper than job numbers suggest. Those junior positions weren't just jobs—they were where people learned to look beneath abstractions. Where you debugged enough low-level issues to build intuition. Where you made enough mistakes to understand why certain patterns exist. AI is eliminating precisely the experience that would teach you to think beyond the abstractions AI creates.

We're facing a future where AI generates increasingly complex systems while the population capable of understanding those systems shrinks. Developers who only work at the abstraction layer—who know how to use tools but not what the tools do—become indistinguishable from AI. The ones who survive understand fundamentals deeply enough to be genuine stewards of AI-generated complexity.

### The Economic Reality: You're Not Fighting the Trend

Before we discuss strategy, let's be brutally honest about the forces at play. The trend toward AI automation isn't a passing fad you can outsmart. It's backed by the most powerful economic incentives imaginable: massive cost reduction, 24/7 availability, perfect consistency, instant scalability. Every incentive structure—from venture capital to corporate profit margins—pushes toward replacing human labor with AI wherever technically feasible.

This isn't a conspiracy. It's economics. When a company can replace a $70,000 junior developer with an AI system costing pennies per task, maintaining profitability demands they do so. When your competitors automate and cut costs while you don't, you lose market share. The capital flowing into AI—hundreds of billions from the world's most sophisticated investors—isn't betting on incremental improvements. They're betting on wholesale transformation of how work gets done.

You cannot beat this trend. You cannot position yourself "against" it. The economic gravity is too strong, the incentives too aligned, the capital too massive. Anyone telling you that simply being human or developing the right skills will insulate you from these forces is selling false hope.

---

## 3. Where Humanness Matters (For Now): Navigating the Uncertain Transition

Despite all the breathless AGI hype, current AI has concrete limitations. Understanding them reveals where human capabilities still provide value during this transition.

The messy, embodied, emotional, imperfect, creative qualities that make you human—these currently provide advantages in specific contexts. Not because AI can never replicate them, but because it hasn't yet. Your lived experience, your physical presence, your ability to form genuine connections, your creative intuition born from seemingly unrelated experiences—these matter today.

The clearest limitation is what Fei-Fei Li calls [spatial intelligence](https://www.youtube.com/watch?v=y8NtMZ7VGmU)—the embodied knowledge you need for physical presence and real-time interaction with unpredictable environments. This is why LLMs (large language models) aren't AGI. It's why [Hinton recommends plumbing over programming](https://www.youtube.com/watch?v=giT0ytynSqg). It's why the ADP data shows nursing assistants and home health aides gaining jobs while software developers lose them. AI can diagnose medical conditions from images, but it can't comfort a frightened patient or adjust based on subtle physical cues. Not yet, anyway.

AI trained on historical data is great at interpolation—finding patterns in what it's seen before. But creative leaps into genuinely new territory? Still hard. When Jeff Dean and Sanjay Ghemawat developed [MapReduce](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) at Google, or when Linus Torvalds created [Git](https://git-scm.com), they weren't making incremental improvements. They reimagined entire classes of problems. AI can analyze existing distributed systems or version control approaches, but pioneering genuinely novel paradigms? Not there yet. And when you need to navigate the uncodified politics of a large organization to actually ship your technical solution? AI is useless. These forms of situated, contextual, novel problem-solving still need humans—but increasingly, experienced ones.

AI performance tracks data availability. Rich, well-structured datasets with well-defined problems? AI excels. Sparse, messy data in poorly understood problem spaces? AI struggles. LLMs are great at natural language, common programming patterns, widely documented procedures—domains with massive training corpora. But [industrial control systems, legacy manufacturing processes, niche scientific instruments, emerging interdisciplinary fields](https://mila.quebec/sites/default/files/media-library/pdf/415051/2025g7aiadoptionfinaleng-1.pdf)? The knowledge base is small, fragmented, held mostly in practitioners' heads. You can't scrape the internet for training data because the knowledge isn't there. This creates a current opening for the Hybrid Specialist—someone who combines CS skills with deep knowledge of a data-poor domain. These domains currently reward human capabilities that are harder to automate: tacit knowledge gained through hands-on experience, intuition developed over years in specific contexts, the ability to synthesize insights across uncodified domains. Examples include [precision agriculture](https://en.wikipedia.org/wiki/Precision_agriculture), where you need both machine learning and the complex biology of crop systems in specific microclimates; industrial maintenance for legacy manufacturing equipment, where documentation is incomplete and expertise lives only in retiring technicians' minds; medical device software, where regulatory requirements, clinical workflows, and embedded systems constraints create problems you can't solve by just knowing how to code; and climate modeling for specific regional applications, where physical intuition about atmospheric dynamics matters as much as computational skill. These domains share a pattern: programming is table stakes, but the real value is code plus context. You're operating where knowledge can't easily be distilled from internet-scale datasets, where physical intuition and domain expertise currently matter, where problems are too specialized for frontier AI labs to prioritize. Plus these fields often have aging workforces and genuine talent shortages.

---

## 4. Strategic Options and Mindset Shifts

Recognizing opportunity isn't the same as seizing it. You need a strategic response combining concrete options with the psychological shifts required to act effectively. The following three areas—strategic positioning, AI fluency, and mindset—work together as a system. Miss any one and the others lose effectiveness.

### Strategic Options Under Uncertainty

The market is splitting workers into two groups, and it's not about smarts or credentials. Some people are becoming more productive through AI. Others are getting displaced. The dividing line appears to be agency and timing. If you wait for traditional on-the-job training, you're competing with AI systems that handle entry-level work faster and cheaper. If you develop capabilities in areas where AI currently struggles, you may become AI-augmented instead of AI-displaced.

The traditional industry path—collect credentials, trade them for a job, learn on the job—is breaking down. The new reality demands demonstrable capability upfront. This doesn't mean skipping college. It means your degree likely won't be enough. You may need to graduate with proof of capability that would normally take years of work to build. What appears to separate those who succeed from those who struggle? Treating AI as a force multiplier from day one while developing capabilities AI can't easily replicate, and creating concrete proof of ability to deliver value. When companies won't invest in training, immediate value matters more.

Academia is changing too. Fields Medal winner Terence Tao has been talking about this—[we're seeing a fundamental shift in how research works](https://www.youtube.com/watch?v=Zu2oET6Xjow). Early career researchers used to spend years mastering sub-skills before leading independent work. Now there's a different path. AI can provide rough ideas when your thinking hits a wall. It can automate the grunt work that eats a junior researcher's time—literature reviews, routine calculations, initial data analysis, draft writing. This means grad students and postdocs could potentially become PIs far earlier than the traditional timeline. Instead of spending years learning to juggle everything—generating ideas, securing funding, managing collaborations—you focus on the core intellectual contribution while AI handles the scaffolding. The bottleneck shifts from mastering routine tasks to developing judgment, taste, the ability to formulate meaningful questions. For those entering academia, this creates both opportunity and pressure: you can make significant contributions earlier, but you have to demonstrate insight AI can't provide.

Whether you go industry or academia, the Hybrid Specialist approach may offer more near-term durability. Combining technical skills with domain expertise in data-poor areas—precision agriculture, legacy industrial systems, medical devices, emerging interdisciplinary fields—creates value that's currently harder for AI to replicate. It's often where the most interesting problems are, where aging workforces create current opportunity, where your work has tangible impact beyond optimizing ad click-through rates.

### Building Your AI Fluency: Understanding the Boundaries

AI fluency isn't optional anymore—it's baseline literacy. But it doesn't mean what the simplified advice suggests. Learning prompt engineering or how to use Copilot isn't enough. Real AI fluency means [understanding what current AI does reliably, what it does unreliably, what it struggles with, and how to architect human](https://anthropic.skilljar.com/ai-fluency-framework-foundations)-AI systems that leverage strengths while compensating for weaknesses.

Remember the abstraction crisis: AI is the ultimate abstraction layer. It can generate working code without you understanding how or why it works. This is precisely the trap. True AI fluency means refusing to treat AI as magic—you need to understand the layers beneath what AI generates. This takes serious time and deliberate practice—not just using AI tools superficially, but deeply understanding their failure modes and boundaries.

More importantly, AI fluency means knowing where you remain essential. You're not competing with AI at what it does well—that's a losing game. The human role is shifting from implementation to higher-level judgment: identifying which problems are worth solving, evaluating whether solutions address actual needs, navigating organizational constraints and stakeholder dynamics that can't be captured in prompts, and critically—understanding what's happening beneath the abstractions AI creates. The goal is becoming someone who can pierce through abstraction layers to verify that AI-generated solutions actually make sense at a fundamental level, not just someone who orchestrates tools they don't understand.

### The Mindset Shift: From Credentials to Capability

Here's what's really changing: the definition of elite itself. Elite used to mean checking boxes that prestigious institutions validated—a Stanford CS degree, a 4.0 GPA, a return offer from Google. That's over. Elite is now the 22-year-old who has been involved in shipping five production systems before graduating, the self-taught developer who can orchestrate AI agents to solve problems senior engineers are still figuring out manually, the hybrid specialist who combines domain expertise with code in ways no bootcamp teaches, the builder who doesn't wait for permission and treats their GitHub as their resume and their shipped products as their credentials.

Kevin Kelly, founding editor of Wired and one of tech's most prescient observers, puts it perfectly in his book [*Excellent Advice for Living: Wisdom I Wish I'd Known Earlier*](https://www.amazon.ca/Excellent-Advice-Living-Wisdom-Earlier/dp/0593654528): "That thing that made you weird as a kid could make you great as an adult—if you don't lose it." This may be the most useful career advice for navigating the AI transition. Your weirdness—those obsessive interests, unusual combinations of knowledge, idiosyncratic ways of thinking—gives you current differentiation. AI excels at the conventional, the well-documented, the statistically common. It currently struggles with the unique intersections that only exist in your particular brain, shaped by your particular life. The kid who was obsessed with both marine biology and programming? That's not scattered interests—that's a potential oceanographic systems specialist. The one who couldn't stop tinkering with old cars while studying CS? That's automotive software expertise that's currently harder to replicate by training on GitHub repos. Your weird provides advantage today—how long that lasts is uncertain, but it's what you have.

This shift is brutal for those who played the old game perfectly. You optimized for every traditional signal—the right school, the right GPA, the right internships. And now the market is saying those signals are increasingly noise. The hardest adjustment might be psychological. The entire education system conditioned you to believe that credentials—a degree from a prestigious university, professional certifications, such as  [CFA](https://en.wikipedia.org/wiki/Chartered_Financial_Analyst) or [CompTIA](https://en.wikipedia.org/wiki/CompTIA)—signal capability and represent your market value. This was never entirely true, but it was true enough to rely on. That compact is broken. The new currency is demonstrated capability, and the sooner you internalize this, the better. This means building things that won't appear on a transcript. Spending weekends on projects with no guaranteed payoff instead of optimizing course grades. Sharing work publicly even when it's imperfect. Thinking of yourself as a craftsperson whose worth is determined by what you make, not the credentials you collect.

This shift is uncomfortable because it trades certainty for ambiguity. A grade is definitive—you know exactly where you stand. A portfolio is ambiguous—it might impress one person and leave another cold. But this ambiguity is valuable. It forces you to develop judgment about what constitutes good work. To seek feedback from practitioners, not professors. To build things that serve real needs instead of satisfying assignment requirements.

## 5. Making Your Bets: A Concrete Action Plan

The career ladder hasn't disappeared—it's changed shape. The bottom rung is gone, but there are ways up if you build your own first step. While most peers follow traditional advice—optimizing for grades, polishing resumes, waiting for the market to improve—you can build capability in areas where demand exceeds supply and ship products that demonstrate value. What follows is a structured approach with three components: developing genuine AI fluency, understanding what production-level work actually means, and building a portfolio that demonstrates both.

### 1. Develop AI Fluency

Treat AI as your force multiplier from day one. The key skill isn't memorizing which models have which quirks—that changes with every release. It's developing systematic judgment about AI capabilities and limitations. Learn to define clear requirements before generating code, so you know what "correct" means. Experiment deliberately: give AI well-specified tasks and poorly-specified ones to understand how problem formulation affects output quality. Study which types of problems AI solves easily (well-documented patterns, clear specifications) versus where it struggles (ambiguous requirements, novel constraints, cross-domain integration). Learn to verify AI output by testing against your requirements, not by hunting for model-specific bugs.

This takes serious investment—[Kevin Kelly's estimate of 1,000 hours](https://news.futunn.com/en/post/41115228/kevin-kelly-s-2024-latest-speech-understanding-ai-from-four) seems about right. With genuine AI fluency, you delegate work AI handles reliably while focusing on what requires human judgment: defining what problems are worth solving, translating ambiguous needs into clear specifications, evaluating whether solutions address actual requirements, making architectural decisions that balance competing constraints. The goal is becoming someone who identifies valuable opportunities, specifies them precisely, directs AI toward them, and critically evaluates results—not someone who competes with AI on implementation speed.

To build this fluency systematically, start with structured resources. Google's [*Prompting Essentials*](https://grow.google/intl/en_ca/enroll-certificates/prompting-essentials-mid) teaches how to write high-quality prompts that get reliable results. The [*Machine Learning Crash Course*](https://developers.google.com/machine-learning/crash-course) provides fundamental understanding of how ML systems work, which helps you reason about what AI can and can't do. Antonio Gullí's [*Agentic Design Patterns*](https://link.springer.com/book/10.1007/978-3-032-01402-3) teaches 21 patterns for building autonomous AI agents with [LangChain](https://www.langchain.com), [Crew AI](https://www.crewai.com), and [Google ADK](https://google.github.io/adk-docs). These aren't just theoretical exercises—they build the mental models you need to architect effective human-AI systems.

### 2. Understand What Production-Level Actually Means

Here's something that trips up almost everyone early in their career: you hear "production-level" or "production-grade" constantly, but nobody ever shows you what that actually looks like. The gap between a class project and something employers value isn't obvious until you've shipped real systems. Let's fix that.

This connects directly to the abstraction crisis. Modern frameworks and AI tools make it trivially easy to build something that works under ideal conditions. AI can generate a working API endpoint in seconds. But that's the illusion—the abstraction hiding all the complexity that makes software actually reliable. Understanding production-level work means seeing through that abstraction to all the things that must be handled for systems to survive in the real world.

A toy system works under ideal conditions—built to demonstrate a concept or complete an assignment with clean inputs and happy paths. A production system is built to survive chaos. The fundamental difference isn't about scale or complexity—it's about thinking through what happens when things go wrong.

The mental shift you need is from "does it work when I test it?" to "can this run unsupervised for months while handling hostile inputs, infrastructure failures, and unexpected load?"

Let's make this concrete. Say you're building a simple script that fetches data from an API and stores it in a database. The toy version is straightforward: write a function that makes an HTTP request, parses the JSON, inserts into the database. Works perfectly when you run it on your laptop. Ship it.

But now ask the production questions. What happens if the API is down when your script runs at 3 AM on a Sunday? You need retry logic with exponential backoff. What if the API returns malformed JSON because they pushed a breaking change? You need defensive parsing that logs failures instead of crashing. What if the database connection drops mid-operation? Handle transactions properly to ensure consistency. What if someone needs to debug why yesterday's data looks wrong? Add structured logging with timestamps and context. What if this needs to run every hour for the next five years? Build in monitoring, alerting, and graceful degradation.

These aren't edge cases. This is normal production life. Databases become unreachable. External APIs change their response format without warning. Traffic spikes 10x because your app hit the front page of Reddit. Users send malformed or actively malicious input. Someone will try to exploit an injection vulnerability. Systems fail, and when they do, you need comprehensive error handling that fails gracefully, logging and monitoring so you can diagnose what went wrong, rigorous input validation, automated tests to catch regressions, deployment automation and rollback capability when things break, and security designed in from the start, not bolted on later.

Here's the uncomfortable part: AI can generate the happy path effortlessly. That's exactly why junior positions are disappearing. The value you add is anticipating failure modes and designing for resilience. When employers say they want someone who can "hit the ground running," they mean someone who thinks through these concerns automatically, not someone who only implements features under ideal conditions.

So how do you develop this mindset? Start with code-level fundamentals. [*Refactoring Guru*](https://refactoring.guru) gives you a solid foundation in refactoring techniques and design patterns—the conceptual basics of how code should be structured. Then move to system-level thinking—Jeff Dean and Sanjay Ghemawat's [*Performance Hints*](https://abseil.io/fast/hints.html) shows you how experienced engineers think about designing systems optimized for specific business goals. For further improvement in understanding real-world challenges for software deployment, explore [Martin Fowler's posts](https://martinfowler.com) which provide deep insights into practical software engineering. It's not just about knowing techniques—it's about developing intuition for when and why they matter, and understanding how architectural choices you make today compound into either smooth operations or maintenance hell years down the line.

This isn't about being paranoid. It's about building systems that reflect how software actually behaves in the real world. When you understand production-level concerns deeply, you're building the fundamental knowledge that lets you evaluate AI-generated solutions critically. You become someone who can look at AI-generated code and immediately spot what's missing—the error handling, the edge cases, the monitoring, the security considerations. You become the human steward who can actually oversee AI-generated complexity, rather than blindly trusting it.

### 3. Build a Portfolio That Demonstrates Capability

With AI fluency and an understanding of production-level work, you're ready to build proof of capability. Create five to ten substantial projects showing you can ship real systems, not toy apps. Make them public, document them thoroughly, solve actual problems.

Focus on projects that showcase orchestrating AI agents to solve complex multi-step problems, building production-grade systems with proper error handling and monitoring, integrating multiple technologies and domains, and delivering complete solutions rather than code fragments. Each project should demonstrate specific competencies employers care about: the ability to handle ambiguity and make architectural decisions, understanding of how to build reliable systems that handle failure gracefully, capacity to integrate multiple technologies and coordinate between them, and evidence you can take a problem from conception to working solution.

Your portfolio should aim to make your resume less relevant. When someone looks at your work, they should see concrete evidence you can deliver value without extensive training. This is your bet that demonstrable capability matters more than credentials—a bet that seems increasingly sound, but comes with no guarantees. It's your attempt to climb past the missing bottom rung, knowing the ladder itself keeps shifting.
